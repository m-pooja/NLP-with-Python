{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Natural Language Processing using Python\n",
    "==================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial you will learn how to implement *basics of natural language processing using python*. You will learn about text processing and some of the very important asspects of Natural Language Processing (NLP) viz. **tokenization, cleaning data, frequency distribution plots, Dispersion plots** apart from the basic **searching, finding similar words** kind of tasks. You will learn how to work with **_NLTK_** library. \n",
    "\n",
    "You will also learn on getting features from the text...\n",
    "\n",
    "\n",
    "_So...Here you GO.... _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to NLP\n",
    "\n",
    "In reality, plain text is the most predominant form of data available today. Text analysis applies analysis of word frequency distributions, pattern recognition, tagging, link and association analysis, sentiment analysis, and visualization. Natural Language Processing is a broad topic, Machine Translation, Summarizing texts, spam detection, sentiment analysis are real big field.\n",
    "Python is readable, fast for prototypes, it has rich library for reading  and writing data, running calculations on the information and creating graphical representations of data sets and list support, it includes a lot of NLP-related libraries viz. **NLTK, textblob, scipy, pandas**... also it has great parsing libraries viz. **Beautifulsoup, scrapy**.\n",
    "> \n",
    "Natural Language Toolkit (NLTK) is a Python API for the analysis of texts written in natural languages, such as English. NLTK is a very popular and old library which comes with a collection of sample texts called corpora (collection of text documents).\n",
    " \n",
    ">NLTK is a leading platform for building Python programs to work with human language data (Natural Language Processing).\n",
    " \n",
    ">NLTK is intended to support research and teaching in NLP or closely related areas, including empirical linguistics, cognitive science, artificial intelligence, information retrieval, and machine learning.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "<dl>\n",
    "   <dt>Accessing data:- </dt>\n",
    "   <dd>Well you need to have an access to your data before starting to process it. You can access and open your data file in many ways.</dd>\n",
    "<dl>\n",
    " \n",
    "   * #### Opening a local file\n",
    " \n",
    "    When you have the data file on your local machine, you can access you file using **_open()_** method and can read into the lines using **_.readline()_**. Don't forget to mention **'r'** mode while opening a file.\n",
    "~~~python\n",
    "f=open('text.txt', 'r')\n",
    "f.readline()\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * #### Opening file from other online source\n",
    "    You might have to process data/text which is available online, then you can fetch the required data like this:-\n",
    "~~~python\n",
    "import requests\n",
    "f_online='http://www.gutenberg.org/files/11111/11111.txt'\n",
    "f_rawtext=requests.get(f_online).text \n",
    "print (f_rawtext)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * #### Opening file from corpus\n",
    "    As mentioned earlier that nltk comes with a collection of sample texts called corpora (collection of text documents). You can access text files from this corpus as well. A complete list is available at http://www.nltk.org/nltk_data/.\n",
    "    \n",
    "   Visit http://www.nltk.org/install.html to **_install nltk_** on your machine.\n",
    "    \n",
    "    You can install **nltk data** as follows:-\n",
    "    \n",
    "~~~python\n",
    "import nltk\n",
    "nltk.download()\n",
    "~~~\n",
    "   \n",
    "   Once you execute the above line of codes, a new window will appear which is a *NLTK Downloader*. You can download the entire collection by using **all**, or just the data required for your project/assignment. You can also download _Gutenberg_ from this link http://www.nltk.org/nltk_data/ .\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Here you can open *Gutenberg* from **nltk.corpus** and create sample text from **'bible-kjv.txt'** file from *Gutenberg*.\n",
    "    \n",
    "~~~python\n",
    "from nltk.corpus import gutenberg\n",
    "textguten = gutenberg.raw(\"bible-kjv.txt\")\n",
    "~~~\n",
    "    \n",
    "   *Note* that **nltk.corpus** needs to be downloaded beforehand if you want to work on corpus data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<dl>\n",
    "   <dt>Text searching:- </dt>\n",
    "   <dd>Before getting into the ways of searching text from text files let's import some text.</dd>\n",
    "<dl>\n",
    " \n",
    "~~~python\n",
    "from nltk.corpus import gutenberg\n",
    "textguten = gutenberg.raw(\"bible-kjv.txt\")\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " You can see that object textguten carries a string. You cannot apply 'Text' funstions on a string.\n",
    "    You need to import **Text** module from _nltk.text_ and you need to fetch words from gutenberg first than raw string.\n",
    "\n",
    "~~~python\n",
    "from nltk.text import Text\n",
    "from nltk.corpus import gutenberg\n",
    "textguten=gutenberg.words(\"bible-kjv.txt\")\n",
    "text=Text(textguten)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   _However if you run this code:-_\n",
    "    \n",
    "~~~python\n",
    "from nltk.book import *\n",
    "~~~\n",
    "  *You will find that the output displays all the **text** files available in the nltk with the index as text1, text2, text 3 and so on till text9 (as it has 9 text files) along with the title and the author, if available, details as sentence (sent1, sent2...sent9). You can directly call text functions like _concordance_, _similar_ on these text files. You can simple get to know about the text by typing in the command line the text name as sown below:-*\n",
    "~~~python\n",
    "text7\n",
    "~~~\n",
    "  \n",
    "\n",
    " _So you can see that **tex7** is a journal named Wall street journal._\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   You have text files now, you can try some text searching commands on it. You can start with 'concordance'. The syntax to use concordance on a text file is as:-\n",
    " \n",
    "__*syntax:-* text_file_name.concordance(\"word_to_be_searched\")__\n",
    "\n",
    " **You have fetched text from Gutenberg and converted that string into _Text_. This text is stored in the variable 'text'. You can now try text searching on this.**\n",
    " \n",
    " Concordance produces all occurrences of a specified word along some context. Using concordance you can search for the incidents where word 'foundation' appears in the text file text1.Concordance allows you to spot lexis (words) in context. \n",
    " \n",
    "~~~python\n",
    "text.concordance(\"God\")\n",
    "~~~\n",
    "\n",
    " *Note that it also specifies the total number of occurrences of the specified word ['Displaying 25 of 4472 matches' as for your word 'God'] at the start.*\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if you want to search for words similar to a given word in the text?\n",
    " \n",
    " **syntax:- text_file_name.similar(\"word_to_be_searched\")**\n",
    "\n",
    " You can try finding similar words of a given word in your _'text'_.\n",
    "~~~python\n",
    "text.similar(\"God\")\n",
    "text.similar(\"monster\")\n",
    "text.similar(\"comit\")\n",
    "text.similar(\"said\")\n",
    "~~~\n",
    "\n",
    " The command .similar() displays words which appear in a similar range of contexts. See output for commands, where similar(\"monster\") and similr(\"comit\").... _no match is found!_.\n",
    " \n",
    "    *Note that we get different results for different texts even for the same word.*\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if you want to look for the contexts which are common for two words? .common_contexts() allows you to do so.\n",
    " \n",
    " **syntax:- name_of_the_file.common_contexts([\"word1\", \"word1\"])**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "text.common_contexts([\"God\", \"thee\"])\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage of collocations is frequent in business and work settings. A collocation is a series of words that happen to appear together often oddly. Like blue sky, feel free, close a deal are collocations. This is how you can look for collocations inside the text:-\n",
    "\n",
    "**syntax:- name_of_the_file.collocations()**\n",
    "~~~python\n",
    "text.collocations()\n",
    "~~~\n",
    "\n",
    " Collocation helps you find **bigrams** that occur more often than you would expect based on the frequency of the individual words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<dl>\n",
    "   <dt>Tokenization:- </dt>\n",
    "   <dd>Tokenizing means splitting your document into lexical chunks.</dd>\n",
    "<dl>\n",
    "\n",
    "Tokenizers are used to **break text strings** and to *find words and punctuation* in a string. \n",
    "  \n",
    "Tokenizing can be done at _word level_ and at _sentence level_. NLTK has a few tokenizers packages. The following sample code requires Punkt sentence tokenization models to be installed. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need a paragraph kind of text to undersand what tokenization does. Well, at the start of this tutorial, you fetched raw data from the same *Gutenberg*. You can make use of that **textguten**!\n",
    "~~~python\n",
    "textguten= gutenberg.raw(\"bible-kjv.txt\")\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "print(sent_tokenize(textguten[:1000]))#considering only first 1000 alphabets from textguten\n",
    "print(word_tokenize(textguten[:1000]))\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<dl>\n",
    "   <dt>Stop Words:- </dt>\n",
    "   <dd>These are common terms which rarely add to the meaning of a sentence for the purposes of NLP and information retrieval. These words are recurring many a times as part of speech and need not be considered for classification.</dd>\n",
    " <dl>\n",
    "\n",
    "In textguten[:1000], you can see, that you get following tokens:-\n",
    "\n",
    "_['**[**', '**The**', 'King', 'James', 'Bible', ']', 'The', 'Old', 'Testament', 'of', 'the', 'King', 'James', 'Bible', 'The', 'First', 'Book', '**of**', 'Moses', ':', 'Called', 'Genesis', '**1:1**', 'In', 'the', 'beginning', 'God', 'created', 'the', 'heaven', 'and', 'the', 'earth', '.', '**1:2**', 'And', 'the', 'earth', 'was', 'without', 'form', ',', 'and', 'void', ';', '**and**', 'darkness', 'was', 'upon', 'the', 'face', 'of', 'the', 'deep', '**.**', 'And', 'the', 'Spirit', 'of', 'God', 'moved', 'upon', 'the', 'face', 'of', 'the', 'waters', '.', '**1:3**', 'And', 'God', 'said', '**,**', 'Let', 'there', 'be', 'light', ':', 'and', 'there', 'was', 'light', '.', '**1:4**', 'And', 'God', 'saw', 'the', 'light', ',', 'that', 'it', 'was', 'good', ':', 'and', 'God', 'divided', 'the', 'light', 'from', 'the', 'darkness', '.', '1:5', 'And', 'God', 'called', 'the', 'light', 'Day', ',', 'and', 'the', 'darkness', 'he', 'called', 'Night', '.', 'And', 'the', 'evening', 'and', 'the', 'morning', 'were', 'the', 'first', 'day', '.', '1:6', 'And', 'God', 'said', ',', 'Let', 'there', 'be', 'a', 'firmament', 'in', 'the', 'midst', 'of', 'the', 'waters', ',', 'and', 'let', 'it', 'divide', 'the', 'waters', 'from', 'the', 'waters', '.', '1:7', 'And', 'God', 'made', 'the', 'firmament', ',', 'and', 'divided', 'the', 'waters', 'which', 'were', 'under', 'the', 'firmament', 'from', 'the', 'waters', 'which', 'were', 'above', 'the', 'firmament', ':', 'and', 'it', 'was', 'so', '.', '1:8', 'And', 'God', 'called', 'the', 'firmament', 'Heaven', '.', 'And', 'the', 'evening', 'and', 'the', 'morning', 'were', 'the', 'second', 'day', '.', '1:9', 'And', 'God', 'said', ',', 'Let', 'the', 'waters', 'under', 'the', 'heav']_\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can create a list of stopwords (like the ones which are _**bold**_) from the above tokens, as follows:-\n",
    "You can eliminate stop words from the _textguten_ either using **list comprehensioon** or by implementing **for loop**.\n",
    "\n",
    "~~~python\n",
    "stop=['[',']','The','of','1:1',':','.','1:2',',','1:3',':','1:4','1:5','1:6','it','1:7','In','1:9','1:8','so','Let']\n",
    "words = word_tokenize(textguten[:1000])\n",
    "clean_textguten= [i for i in words if not i in stop]#using list comprehension\n",
    "~~~\n",
    "~~~python\n",
    "#using for loop\n",
    "clean_textguten = []\n",
    "for i in words:\n",
    "    if i not in stop:\n",
    "        clean_textguten.append(i)\n",
    "print(clean_textguten)\n",
    "~~~\n",
    "\n",
    "After this, you have got clean text (*clean_textguten*)!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the output does not contain the tokens which were present in the list of stopwords.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Try the following code!_\n",
    "Another way of creating stopwords is by using stopwords module from nltk.corpus as follows:-\n",
    " \n",
    "~~~python\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('English')) \n",
    "~~~\n",
    "\n",
    "_Here, you are creating a set of 'English' stop words. Well if you are working on say French text, you can create French stopwords as well._\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<dl>\n",
    "   <dt>Lexicon Normalization:-</dt>\n",
    "   <dd>To perform normalization that mainly involves eliminating punctuation, converting the entire text into lowercase or uppercase, converting numbers into words, expanding abbreviations, canonicalization of text, and so on. As text is fetched in the form of a string, .lower() and .upper() can be applied on it for converting text into lowercase or uppercase. The commonly used normalization methods are stemming and lemmatization.</dd>\n",
    " <dl> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<dl> \n",
    "    <dt>Stemming:- </dt>\n",
    "        <dd>Words need to be stemmed to retrieve their radicals, so that various forms derived from a stem would be taken as the same when counting word frequency. Stemming is used to remove morphological affixes from words, leaving only the word stem. For instance, words “update\", “updated\" and “updating\" would all be stemmed to “update\".</dd>\n",
    "</dl>\n",
    "\n",
    "For stemming, you need to import some stemmer from *nltk.stem module*.\n",
    "Then create an object for that stemmer. \n",
    "And then you can find the stems for all the words in the *clean text* using for loop.\n",
    "\n",
    "~~~python\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "stemtext=[]\n",
    "for i in clean_textguten:\n",
    "    p=ps.stem(i)\n",
    "    stemtext.append(p)\n",
    "print(stemtext)\n",
    "~~~\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<dl>\n",
    "<dt>Lemmatization:-</dt>\n",
    "\n",
    "<dd>This operation is similar to stemming. Let's lemmatize a few words. First you need to import a lemmatizer. You can use WordNetLemmatizer from nltk.stem package. Then create an instance of that lemmatizer and then use .lemmatize() for getting the stems of words, as follows:-</dd>\n",
    "</dl> \n",
    "~~~python\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lr = WordNetLemmatizer()\n",
    "lemtext=[]\n",
    "for i in clean_textguten:\n",
    "    p=lr.lemmatize(i)\n",
    "    lemtext.append(p)\n",
    "print(lemtext)\n",
    "~~~\n",
    "\n",
    "Here, you are importing WordNetLemmatizer, then **lr** object is created for this lemmatizer. And you are creating a list of lemmatized word (_lemtext_) for the words present in _clean_textguten_.\n",
    "\n",
    "  * Note that it returns the input word unchanged if it cannot be found in WordNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to Features (Feature Engineering on text data)\n",
    "<dl>\n",
    "   <dt>Part of Speech Tagging:- </dt>\n",
    "   <dd>This process converts tokenizes words into a list of containing word and its corresponding POS. A part-of-speech tag signifies whether the word is a noun, adjective, verb, and so on.</dd>\n",
    "<dl>\n",
    "\n",
    "You can start with tagging POS to your text!\n",
    "\n",
    "First of all, you need to tokenize the string in to words using word_tokenize, (_if required_) and then you need create a list called 'words' of the tokens so created. \n",
    "\n",
    "Using pos_tag from nltk, you can tag tokens to their respective POS. Well, Later you can print those tags.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, you have your clean text (clean_textguten), which is already in the form of tokens, so you will not require to tokenize it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "tagged_textguten = nltk.pos_tag(clean_textguten)\n",
    "print(tagged_textguten[:2])#printing results for only first two instances!\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear from the output that 'the' is determiner as indicated by DT and 'Project' is 'Noun, Proper, Singular' as indicated by NNP. Note that tagger creates a list of tuple containing word and its corresponding POS.\n",
    "\n",
    "**POS tagging is an essential step prior to making chunks as chunker cannot recognize extract phrases from a sentence without the part-of-speech tags.**\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<dl>\n",
    "<dt>Chunking:- </dt>\n",
    "<dd>\n",
    "Chunks are the minimal units that can be processed or we can say that chunks are the non-overlapping linguistic groups. Chunking is a kind of shallow parsing. Chunking is not possible to do until we have done tagging.</dd>\n",
    "</dl>\n",
    "You need to define a chunk grammar i.e. You need to create a rule (regular expression based on tags) for the entity to be extracted.\n",
    "To do this, you're going to use regular expressions. A regular expression is a set of characters used to define a search pattern. Let's say you want to search for words begning with 'a', you need to create a regex which will be 'a\\w+' in your case.\n",
    "Where 'a' at the start of regex means will only match words that begin with 'a'; '\\w' is a special character and it will match any alphanumeric A-z, a-z, 0-9, along with underscores;'+' means the regex can appear many times in strings.\n",
    "\n",
    "for example:- grammar = \"NP: {\\<DT\\>\\?<JJ>}\"\n",
    "This rule says that an NP chunk should be formed whenever the chunker finds an optional determiner (DT) followed by any number of adjectives (JJ).\n",
    "then, you can pass this grammar to a RegularParser and parse the sentence we want to chunk for. \n",
    "Regular Expression meanings:-\n",
    "+ = match 1 or more\n",
    "? = match 0 or 1 repetitions.\n",
    "* = match 0 or MORE repetitions\t  \n",
    ". = Any character except a new line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "~~~python\n",
    "for i in clean_textguten[:2]:\n",
    "    chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "    chunkParser = nltk.RegexpParser(chunkGram)\n",
    "    chunked = chunkParser.parse(tagged)\n",
    "    chunked.draw()           \n",
    "    print(chunked) \n",
    "~~~\n",
    "\n",
    "Note that the chunkgram n the above example code means:-\n",
    "    <RB.?>* = \\\"0 or more of any tense of adverb,\n",
    "    <VB.?>* = \\\"0 or more of any tense of verb,\n",
    "    <NNP>+ = \\\"One or more proper nouns,\n",
    "    <NN>? = \\\"zero or one singular noun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<dl>\n",
    "<dt>Named Entity Recognition:- </dt>\n",
    "<dd>Named entity is a real-world object, such as persons, locations, organizations, products, etc., that can be denoted with a proper name (https://en.wikipedia.org/wiki/Named_entity). Named entity recognition is a way of extracting most common entities such as people, places, things, locations, monetary figures, and more. With named entity recognition you can easily locate proper names of people, organizations, locations, or other entities from the text. After tagging you can use __nltk.ne_chunk()__, nltk provides a classifier trained to recognize named entities, accessed with the function nltk.ne_chunk(). \n",
    "</dd>\n",
    "</dl>\n",
    "~~~python\n",
    "for i in clean_text[:2]:\n",
    "    ner = nltk.ne_chunk(tagged, binary=True)\n",
    "    ner.draw()\n",
    "~~~\n",
    "    \n",
    "   Note that if you set binary=True, then named entities are just tagged as NE; otherwise, the classifier adds category labels such as Person, Organization, and GPE.\n",
    "\n",
    "\n",
    "~~~python\n",
    "for i in clean_text[:2]:\n",
    "    ner = nltk.ne_chunk(tagged)\n",
    "    ner.draw()\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<dl>\n",
    "<dt>TF – IDF :- <dt>\n",
    "<dd>A term-document matrix represents the relationship between terms and documents, where each row stands for a term or words as present in the data sheet and each column for a document, and an entry is the number of occurrences of the term in the document. </dd>\n",
    "</dl>\n",
    "Given a word, a document and a collection of documents, Vocabulary of the document is the number of unique/different words that document contains. Or we can say that The vocabulary of a text is just the set of tokens that it uses.\n",
    "\n",
    "   Term Frequency is the count of words in the document **(vocab) / length of the document**.\n",
    "\n",
    "   Inverse Document Frequency is the **log of length of collection / count of document_containing_term in the collection**.\n",
    "\n",
    "   **Term Frequency – Inverse Document Frequency = tf * idf **\n",
    "\n",
    "  Note nltk does not have anything to find _Term Frequency – Inverse Document Frequency_ directly. However you can do so using **len()** method. This is explained under the head of Lexical richness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<dl>\n",
    "<dt>Lexical richness:-</dt>\n",
    " <dd>Lexical richness means number of the number of distinct words in the document.</dd>\n",
    "</dl>\n",
    "\n",
    "Function for finding lexical richness of the text:-\n",
    "\n",
    "~~~python\n",
    "def lexical_richness(text):\n",
    "   return len(set(text)) / len(text)\n",
    "~~~\n",
    "Function for finding percentage of the text is taken up by a specific word:-\n",
    "~~~python\n",
    "def percentage(word, text):\n",
    "   return (100 * text.count(word) / len(text))\n",
    "~~~\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can call both functions for your text _textguten_\n",
    "\n",
    "~~~python\n",
    "print ('Lexical richness of the text: '+str(lexical_richness(textguten)))\n",
    "print ('Percentage: '+ str(percentage('only',(textguten))))\n",
    "~~~\n",
    "\n",
    "_Using **math.log()**, you can easily find the IDF_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<dl>\n",
    "<dt>Frequency Distribution:-</dt>\n",
    "<dd>Frequency distribution notify about the frequency of each vocabulary item in the text. </dd>\n",
    "</dl>\n",
    "\n",
    "Using following code you can find the frequency distribution of *textguten*. You need to import *FreqDist* beforehand from *nltk*.\n",
    "\n",
    "~~~python\n",
    "from nltk import FreqDist\n",
    "fdist_textguten = FreqDist(textguten)\n",
    "~~~\n",
    "\n",
    "   Note that output is a _dictionary_ containing every _vocabulary item_ with the value of its _occurrence_. You can also display this dictionary using _print()_ command.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~~python\n",
    "print(fdist_textguten)\n",
    "~~~\n",
    "\n",
    "You can also find most common 10 vocab items of this text.\n",
    "~~~python\n",
    "fdist_textguten.most_common(10)\n",
    "~~~\n",
    "   Well, you can also plot frequency distribution graph. If you wish to plot frequency distribution, try following code, which plots frequency distribution of 10 most common vocab items.\n",
    "\n",
    "~~~python\n",
    "fdist_textguten.plot(10, cumulative=True)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<dl>\n",
    "<dt>Dispersion plot:-</dt>\n",
    "<dd>Dispersion plots determines the location of a word in the text: how many words from the beginning it appears.</dd>\n",
    "</dl>\n",
    "  \n",
    "   With dispersion plot you can find the positional information of a word.\n",
    "   \n",
    "   **    \"syntax:- name_of_the_file.dispersion_plot([\\\"word1\\\", \\\"word2\\\", \\\"word3\\\"...])\"\n",
    "**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "textguten=gutenberg.words(\"bible-kjv.txt\")\n",
    "text=Text(textguten)#dispersion_plot does not work on strings!\n",
    "text.dispersion_plot([\"God\", \"James\", \"Bible\"]) \n",
    "text.dispersion_plot(['Testament', 'created'])\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   With the help of these plots you can easily visualize that how many times a word has appeared in your text (*textguten*).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<dl>\n",
    "<dt>Finding synonym:-</dt>\n",
    "<dd>WordNet is a lexical database for the English language and can be used to find the meanings of words, synonyms, antonyms, and more.\n",
    "</dd>\n",
    "</dl>\n",
    "For finding synonyms of a word, you first need to import **wordnet** from nltk and then you can call **syssets()** on the word for which you wish to know the synomym."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well to fins a synonym you need to have a word, for which you can find it's synonym!\n",
    "**clean_textguten** contains words only from textguten. \n",
    "You can make use of it here or you can tokenize textguten into words for finding synonyms.\n",
    "\n",
    "\n",
    "~~~python\n",
    "#word_textguten=word_tokenize(textguten[:100])#use for tokenizing!\n",
    "from nltk.corpus import wordnet\n",
    "syns=[]\n",
    "for i in clean_textguten[:2]:#considering only two words!\n",
    "   syns.append( wordnet.synsets(i))\n",
    "print(syns)\n",
    "~~~\n",
    "\n",
    "The output for the above command will be a list of synonyms for each word.\n",
    "_You can easily look inside this list of synonyms using indexing as follows:-\n",
    "print(syns[0].name()), print(syns[0].definition()), print(syns[0].examples())_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Creating list of synonyms and antonyms of a word by using **.lemmas()** and **.antonyms()**.\n",
    "\n",
    "~~~python\n",
    "synonyms = []\n",
    "antonyms = []\n",
    "for i in clean_textguten[:2]:\n",
    "   s=wordnet.synsets(i)\n",
    "    for syn in s:\n",
    "        for l in syn.lemmas():\n",
    "            synonyms.append(l.name())\n",
    "            if l.antonyms():\n",
    "                antonyms.append(l.antonyms()[0].name())\n",
    "print(set(synonyms))\n",
    "print(set(antonyms))\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coding skills you have learned are going to be tremondously helpful and will be relevant to a significant part of data, called Unstructured Data and incorporates a lot of content.\n",
    "\n",
    "Thanks for reading, and I hope to see you in the next tutorial!, where you will learn to classify unstructured data using models like Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
